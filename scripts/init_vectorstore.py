"""
LangGraph Catalyst - Initial Data Loading Script

ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€åˆ†å‰²ã—ã¦ã€Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã—ã¾ã™ã€‚
"""

import argparse
import logging
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.features.rag.crawler import (  # noqa: E402
    crawl_github_repo,
    crawl_langchain_blog,
    crawl_langchain_docs,
    crawl_langgraph_docs,
)
from src.features.rag.vectorstore import ChromaVectorStore  # noqa: E402
from src.utils.helpers import split_documents  # noqa: E402

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="Initialize vector store with LangGraph documentation"
    )
    parser.add_argument(
        "--max-docs-pages",
        type=int,
        default=10,
        help="Maximum pages to crawl from official docs",
    )
    parser.add_argument(
        "--max-blog-articles",
        type=int,
        default=5,
        help="Maximum articles to crawl from blog",
    )
    parser.add_argument(
        "--skip-github", action="store_true", help="Skip GitHub repository crawling"
    )
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="Delete existing collection and recreate",
    )
    parser.add_argument("--verbose", action="store_true", help="Verbose logging")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    print("=" * 70)
    print("LangGraph Catalyst - Vector Store Initialization")
    print("=" * 70)
    print()

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
    print("ğŸ“¦ Initializing vector store...")
    try:
        vectorstore = ChromaVectorStore()
        print(f"âœ… Vector store initialized: {vectorstore.collection_name}")
        print(f"   Persist directory: {vectorstore.persist_directory}")
        print()
    except Exception as e:
        print(f"âŒ Failed to initialize vector store: {e}")
        return 1

    # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã™ã‚‹å ´åˆ
    if args.recreate:
        print("ğŸ—‘ï¸  Deleting existing collection...")
        try:
            vectorstore.delete_collection()
            print("âœ… Collection deleted")
            # å†åˆæœŸåŒ–
            vectorstore = ChromaVectorStore()
            print("âœ… Collection recreated")
            print()
        except Exception as e:
            print(f"âš ï¸  Warning: Could not delete collection: {e}")
            print()

    # æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’ç¢ºèª
    existing_count = vectorstore.get_collection_count()
    print(f"ğŸ“Š Current documents in store: {existing_count}")
    print()

    all_documents = []

    # 1. LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«
    print("ğŸŒ Crawling LangGraph official documentation...")
    try:
        docs_langgraph = crawl_langgraph_docs(
            max_pages=args.max_docs_pages, include_api_reference=True
        )
        print(f"âœ… Crawled {len(docs_langgraph)} pages from LangGraph docs")
        all_documents.extend(docs_langgraph)
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to crawl LangGraph docs: {e}")
    print()

    # 2. LangChainå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆé–¢é€£ãƒšãƒ¼ã‚¸ï¼‰ã®ã‚¯ãƒ­ãƒ¼ãƒ«
    print("ğŸ”— Crawling LangChain official documentation (related pages)...")
    try:
        docs_langchain = crawl_langchain_docs(max_pages=10)
        print(f"âœ… Crawled {len(docs_langchain)} pages from LangChain docs")
        all_documents.extend(docs_langchain)
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to crawl LangChain docs: {e}")
    print()

    # 3. LangChain Blogã®ã‚¯ãƒ­ãƒ¼ãƒ«
    print("ğŸ“ Crawling LangChain Blog...")
    try:
        docs_blog = crawl_langchain_blog(tag="langgraph", max_articles=args.max_blog_articles)
        print(f"âœ… Crawled {len(docs_blog)} articles from blog")
        all_documents.extend(docs_blog)
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to crawl blog: {e}")
    print()

    # 4. GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ«
    if not args.skip_github:
        print("ğŸ™ Crawling GitHub repository...")
        try:
            docs_github = crawl_github_repo(
                repo="langchain-ai/langgraph", include_examples=True, include_tests=False
            )
            print(f"âœ… Crawled {len(docs_github)} documents from GitHub")
            all_documents.extend(docs_github)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to crawl GitHub: {e}")
        print()

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆ
    if not all_documents:
        print("âŒ No documents were crawled. Exiting.")
        return 1

    print(f"ğŸ“š Total documents crawled: {len(all_documents)}")
    print()

    # 5. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
    print("âœ‚ï¸  Splitting documents into chunks...")
    try:
        splits = split_documents(all_documents)
        print(f"âœ… Split into {len(splits)} chunks")
    except Exception as e:
        print(f"âŒ Failed to split documents: {e}")
        return 1
    print()

    # 6. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
    print("ğŸ’¾ Adding documents to vector store...")
    try:
        result = vectorstore.add_documents(splits, batch_size=50)

        print("âœ… Document addition completed:")
        print(f"   Added: {result['added_count']} chunks")
        print(f"   Failed: {result['failed_count']} chunks")
        print(f"   Total in store: {result['total_documents_in_store']} chunks")
        print(f"   Status: {result['status']}")
    except Exception as e:
        print(f"âŒ Failed to add documents: {e}")
        return 1
    print()

    # 7. ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆ
    print("ğŸ” Testing with sample query...")
    try:
        test_query = "What is LangGraph?"
        results = vectorstore.similarity_search(test_query, k=3)

        print(f"âœ… Sample query: '{test_query}'")
        print(f"   Found {len(results)} relevant documents")

        if results:
            print("\n   Top result:")
            print(f"   - Title: {results[0].metadata.get('title', 'N/A')}")
            print(f"   - Source: {results[0].metadata.get('source', 'N/A')}")
            print(f"   - Preview: {results[0].page_content[:100]}...")
    except Exception as e:
        print(f"âš ï¸  Warning: Sample query failed: {e}")
    print()

    print("=" * 70)
    print("âœ… Vector store initialization completed successfully!")
    print("=" * 70)
    print()
    print("You can now run the Streamlit app:")
    print("  streamlit run src/app.py")
    print()

    return 0


if __name__ == "__main__":
    sys.exit(main())
